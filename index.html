<!DOCTYPE HTML>
<html lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-JKQK3E33B3"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-JKQK3E33B3');
  </script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Seong Joon Oh</title>

  <meta name="author" content="Seong Joon Oh">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="pictures/personal_round.png">

</head>

<body>
  <div class="container">
    <div class="row" style="margin-top: 10px;">
      <div class="col-sm-4 name-column" style="min-width: 266px;">
        <p style="text-align:center">
          <name>Seong Joon Oh</name>
        </p>
      </div>
      <div class="col-sm-8 name-column text-right" style="min-width: 266px;">
        <p style="text-align:right">
          <a href="https://scalabletrustworthyai.github.io/"><img src="pictures/stai_logo.png" alt="stai_logo" class="institute-logo"></a>
            &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
          <a href="https://tue.ai/"><img src="pictures/logo_tueai.svg" alt="tueai_logo" class="institute-logo"></a>
            &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
          <a href="https://uni-tuebingen.de/"><img src="pictures/logo_uni_tue.svg" alt="logo_uni_tue" class="institute-logo"></a>
            &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
          <a href="https://parameterlab.de/"><img src="pictures/logo_parameterlab.png" alt="logo_uni_tue" class="institute-logo"></a>
        </p>
      </div>
    </div>
    <div class="row common-rows">

      <div class="col-xs-12 col-sm-8 personal-column">
        <p>I am a professor at the <a href="https://tuebingen.ai/">University of Tübingen</a> leading the group on <a href="https://scalabletrustworthyai.github.io/">Scalable Trustworthy AI (STAI)</a>.
          In addition to my main job, I advise <a href="https://www.parameterlab.de/">Parameter Lab</a>.
          I am generally interested in training reliable models (<emph>e.g.</emph> explainable, robust, and probabilistic models) and obtaining the necessary human supervision and guidance in a cost-effective way.
        </p>
        <p>
          I have been a research scientist at <a href="https://github.com/naver-ai">NAVER AI Lab</a> for 3.5 years.
          I received my PhD in computer vision and machine learning at <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning">Max-Planck Institute for Informatics</a> in 2018,
          under the supervision of <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a> and <a href="https://cispa.saarland/group/fritz/">Mario Fritz</a>,
          with a focus on the privacy and security implications of CV and ML (<a href="https://publikationen.sulb.uni-saarland.de/handle/20.500.11880/27146">Thesis</a>).
          I received the Master of Mathematics with <a href="https://en.wikipedia.org/wiki/Part_III_of_the_Mathematical_Tripos">Distinction</a> in 2014 and
          Bachelor of Arts in Mathematics as a <a href="https://en.wikipedia.org/wiki/Wrangler_(University_of_Cambridge)">Wrangler</a> in 2013,
          both at University of Cambridge.
        </p>
        <p>
          I started compiling the <a href="https://github.com/coallaoh/Principles">principles for life and research &#127822;</a>.
        </p>

        <p style="text-align:center">
          <a href="mailto:coallaoh@gmail.com">Email</a> &nbsp/&nbsp
          <a href="https://scholar.google.com/citations?user=kmXOOdsAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
          <a href="https://www.linkedin.com/in/seong-joon-oh-32113479/">LinkedIn</a> &nbsp/&nbsp
          <a href="https://twitter.com/coallaoh/">Twitter</a> &nbsp/&nbsp
          <a href="https://github.com/coallaoh/">Github</a>
        </p>

      </div>
      <div class="col-xs-12 col-sm-4 personal-column">
        <img alt="profile photo" src="pictures/personal_round.png" class="personal-photo">
      </div>
    </div>
  </div>

  <div class="container">
    <div class="row section-heading-rows">
      <h3>Updates</h3>
    </div>
    <div class="row">
      <p>
        <ul>
          <li>16 October 2023. I'm teaching the <a href="https://scalabletrustworthyai.github.io/courses/tml_winter_2324/">Trustworthy ML course</a> @ University of Tübingen. Materials from last year have been published as a <a href="https://trustworthyml.io/">book</a>!</li>
          <li>22 September 2023. 4 paper accepted @ NeurIPS 2023. A <a href="#elisa2023neurips">poster</a> with Elisa, a <a href="#siwon2023neurips">spotlight</a> with Siwon, Parameter Lab, and Naver, a <a href="#teney2023neurips">spotlight</a> with Damien, and a <a href="#kirchhof2023neuripsdb">poster</a> with Michael.</li>
          <li>24 July 2023. The <a href="#kirchhof2023uaieai">URL benchmark</a> has received the Best Student Paper Award at the UAI workshop on Epistemic AI.</li>
          <li>21 July 2023. I have started my affiliation with <a href="https://www.parameterlab.de/">Parameter Lab</a>. Through this construct, I collaborate with <a href="https://navercorp.com/en">Naver</a> on foundational models and bridge industrial and academic research efforts.</li>
          <li>14 July 2023. 2 papers accepted @ ICCV'23: <a href="#han2023iccv">Neglected Free Lunch</a> and <a href="#nam2023iccv">Scratching the Back</a>.
        </ul>
      </p>
    </div>
  </div>


  <div class="container">
    <div class="row section-heading-rows">
      <h3>Research</h3>
    </div>
    <div class="row">
      <p>
        I have tried to push certain fronts in ML research to make models truly useful and deployable in real life. They can be grouped into a few keywords.
        <br><br>
        <span style="background-color:#ff9aa2">Robustness</span>.
        Changes in the input distribution shall not disrupt the model's predictive power. Ideally, a model should be robust against the shifts in input domain (<em>e.g.</em> natural and adversarial perturbations) and confounders (<em>e.g.</em> fairness).
        <br><br>
        <span style="background-color:#ffdac1">Uncertainty</span>.
        A model should know when it is going to get it wrong. This allows the users and downstream systems to make sensible and safe decisions based on the estimated confidence levels.
        <br><br>
        <span style="background-color:#ffffce">Human Annotation</span>.
        An integral part of training a high-performance model is the human supervision. I have sought cost-effective ways to extract useful supervisory signals from humans.
        <br><br>
        <span style="background-color:#b5ead7">Privacy & Security</span>.
        There are different privacy and security angles with which ML can be analyzed. One may question the "stealability" of a black-box model as an IP; one may also question the privacy guarantees for user data in the federated learning setup.
        Still others may wonder whether certain level privacy is achievable at all on internet, with the increasing volume of user data online and more widespread use of machine learning algorithms to process such data.
        <br><br>
        <span style="background-color:#c7ceea">Explainability</span>.
        Humans do not use systems that are not trustworthy. Humans thus find it hard to deal with systems that do not explain the rationale. Explanations are an integral part of trustworthiness. A model must provide a faithful reasoning for its decisions, ideally paving way to practical action items to improve the model.
        <br><br>
        <span style="background-color:#e2c7e5">Evaluation</span>.
        Correct evaluation is undoubtably important in research and industrial applications, yet it is surprisingly difficult. I have cleaned up benchmarks and evaluation protocols in a few domains.
        <br><br>
        <span style="background-color:#C9D3D8">Large-Scale ML</span>.
        Some of the methodologies I have been involved in are designed for large-scale ML. They typically require minimal changes to the original ML system but bring consistent gains across the board.
        <br><br>
        See <a href="https://docs.google.com/presentation/d/1BVyjRE-jq3cuVl9NM29WeFi6eUBTtPi6KzKRQufJr4g/edit?usp=sharing">slides</a> and <a href="https://www.youtube.com/watch?v=oiFS2N1ocTA">video</a> (3 Aug 2022) for an overview of the past researches and future research ideas for the scalable trustworthy AI.
      </p>
    </div>
  </div>

  <div class="container">
    <div class="row section-heading-rows">
      <h3>Publications</h3>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
          <img src="pictures/evgenii2024ralm.png" alt="evgenii2024ralm" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#c7ceea">Explainability</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <span style="background-color:#C9D3D8">Large-Scale ML</span>
        <br>
        <a href="https://arxiv.org/abs/2404.16032" id="evgenii2024ralm">
          <papertitle>Studying Large Language Model Behaviors Under Realistic Knowledge Conflicts
          </papertitle>
        </a>
        <br>
        <a href="https://kortukov.github.io/">Evgenii Kortukov</a>,
        <a href="https://scalabletrustworthyai.github.io/member/alex/">Alexander Rubinstein</a>,
        <a href="https://elisanguyen.github.io/">Elisa Nguyen</a>,
        <strong>Seong Joon Oh</strong>.
        <br>
        <em>arXiv</em>, 2024.
        <br>
        <a href="data/evgenii2024ralm.txt">Bibtex</a>
        <p>Retrieval augmented generation (RAG) promises more trustworthy outputs from large language models (LLMs). RAG first retrieves relevant documents from a DB and includes them in the context for subsequent generation. However, RAG does not come with guarantee. Eventually, LLM decides whether to use the new information in retrieved document or to stick to the original information in the pre-training data. We present a study on this knowledge conflict.
        </p>
      </div>
    </div>
    
    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
          <img src="pictures/balint2024disentanglement.png" alt="balint2024disentanglement" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ffdac1">Uncertainty</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <br>
        <a href="https://arxiv.org/abs/2402.19460" id="balint2024disentanglement">
          <papertitle>Benchmarking Uncertainty Disentanglement: Specialized Uncertainties for Specialized Tasks
          </papertitle>
        </a>
        <br>
        <a href="https://bmucsanyi.github.io/">Bálint Mucsányi</a>,
        <a href="https://www.hci.uni-tuebingen.de/chair/team/michael-kirchhof">Michael Kirchhof</a>,
        <strong>Seong Joon Oh</strong>.
        <br>
        <em>arXiv</em>, 2024.
        <br>
        <a href="data/balint2024disentanglement.txt">Bibtex</a>
        <p>After the <a href="#tml2223">Trustworthy Machine Learning</a> course, <a href="https://bmucsanyi.github.io/">Bálint</a> has investigated the relationships between different types of uncertainty in machine learning models. He found that many methods claiming to measure specific uncertainties had not been thoroughly verified. After the experiments, we concluded that these methods hardly achieved their claimed goals. This revelation is crucial for the uncertainty estimation community, where they try to understand and disentangle different uncertainty types.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
          <img src="pictures/kirchhof2024pretrained.png" alt="kirchhof2024pretrained" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ffdac1">Uncertainty</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <span style="background-color:#C9D3D8">Large-Scale ML</span>
        <br>
        <a href="https://arxiv.org/abs/2402.16569" id="kirchhof2024pretrained">
          <papertitle>Pretrained Visual Uncertainties
          </papertitle>
        </a>
        <br>
        <a href="https://www.hci.uni-tuebingen.de/chair/team/michael-kirchhof">Michael Kirchhof</a>,
        <a href="https://www.linkedin.com/in/mark-collier-aa446032/">Mark Collier</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://www.edu.sot.tum.de/hctl/prof-dr-enkelejda-kasneci/">Enkelejda Kasneci</a>.
        <br>
        <em>arXiv</em>, 2024.
        <br>
        <a href="data/kirchhof2024pretrained.txt">Bibtex</a>
        <p>Uncertainty estimation so far had to be learned from scratch for each new task. We introduce a new approach that allows us to train uncertainty estimation on a large, general dataset and then apply it to new, specific tasks. We focus on practicality and efficiency. Our approach captures inherent uncertainty in the data, separate from uncertainty due to limited knowledge.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
          <img src="pictures/ankit2024star.png" alt="ankit2024star" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ff9aa2">Robustness</span>
        <span style="background-color:#ffdac1">Uncertainty</span>
        <br>
        <a href="https://arxiv.org/abs/2403.07968" id="ankit2024star">
          <papertitle>Do Deep Neural Network Solutions Form a Star Domain?
          </papertitle>
        </a>
        <br>
        <a href="https://www.linkedin.com/in/ankit-sonthalia-2a05b4202/">Ankit Sonthalia</a>,
        <a href="https://scalabletrustworthyai.github.io/member/alex/">Alexander Rubinstein</a>,
        <a href="https://ehsanabb.github.io/">Ehsan Abbasnejad</a>,
        <strong>Seong Joon Oh</strong>.
        <br>
        <em>arXiv</em>, 2024.
        <br>
        <a href="data/ankit2024star.txt">Bibtex</a>
        <p>For deep neural networks, understanding the solution set, or the set of parameters with low loss values, is crucial. It has been conjectured that the solution set forms a convex set, modulo parameter permutations. The conjecture has met several counterexamples. Instead, we propose that the solution set forms a <em>star domain</em>: there exists a central "star model" connected to all other solutions. This is weaker and more relaxed than the convex-set conjecture, but does not contradict empirical findings. 
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
          <img src="pictures/martin2024trap.png" alt="martin2024trap" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#b5ead7">Privacy & Security</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <span style="background-color:#C9D3D8">Large-Scale ML</span>
        <br>
        <a href="https://arxiv.org/abs/2402.12991" id="martin2024trap">
          <papertitle>TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification
          </papertitle>
        </a>
        <br>
        <a href="https://gubri.eu/">Martin Gubri</a>,
        <a href="https://dennisulmer.eu/">Dennis Ulmer</a>,
        <a href="https://hwaranlee.github.io/">Hwaran Lee</a>,
        <a href="https://sangdooyun.github.io/">Sangdoo Yun</a>,
        <strong>Seong Joon Oh</strong>.
        <br>
        <em>arXiv</em>, 2024.
        <br>
        <a href="data/martin2024trap.txt">Bibtex</a>
        <p>Large language models (LLM) and surrounding services come with their own rules about <em>who</em> can use them and <em>how</em> they should be used. These rules are important to protect the company's work and to prevent misuse. Now, given a new LLM-based chatbot service, it's important to find out the underlying LLM in order to check the compliance with the rules attached to each LLM. Here's our method for doing this: We ask the chatbot a very specific question that only one company's machine will answer in a certain way. It's like asking a friend a secret question only they would know the answer to. If the machine answers the question the way we expect, we know it's based on a specific LLM.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
          <img src="pictures/dennis2024apricot.png" alt="dennis2024apricot" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ffdac1">Uncertainty</span>
        <span style="background-color:#C9D3D8">Large-Scale ML</span>
        <br>
        <a href="https://arxiv.org/abs/2403.05973" id="dennis2024apricot">
          <papertitle>Calibrating Large Language Models Using Their Generations Only
          </papertitle>
        </a>
        <br>
        <a href="https://dennisulmer.eu/">Dennis Ulmer</a>,
        <a href="https://gubri.eu/">Martin Gubri</a>,
        <a href="https://hwaranlee.github.io/">Hwaran Lee</a>,
        <a href="https://sangdooyun.github.io/">Sangdoo Yun</a>,
        <strong>Seong Joon Oh</strong>.
        <br>
        <em>arXiv</em>, 2024.
        <br>
        <a href="data/dennis2024apricot.txt">Bibtex</a>
        <p>We can't trust large language model (LLM) outputs. One of the reasons is that it doesn't always generate reliable confidence estimates. One could look into the model likelihoods, but even that is infeasible for many black-box models. We show here that it's possible to train a lightweight external model to infer an LLM's internal confidence based only on the prompt and answers from the LLM (purely black box).
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
          <img src="pictures/elisa2023neuripsxaiw.png" alt="elisa2023neuripsxaiw" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#c7ceea">Explainability</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <br>
        <a href="https://arxiv.org/abs/2310.20477" id="elisa2023neuripsxaiw">
          <papertitle>Exploring Practitioner Perspectives On Training Data Attribution Explanations
          </papertitle>
        </a>
        <br>
        <a href="https://elisanguyen.github.io/">Elisa Nguyen</a>,
        <a href="https://scalabletrustworthyai.github.io/member/evgenii/">Evgenii Kortukov</a>,
        <a href="https://jyskwon.github.io/">Jean Y. Song</a>,
        <strong>Seong Joon Oh</strong>.
        <br>
        <em>NeurIPS XAI in Action Workshop</em>, 2023.
        <br>
        <a href="data/elisa2023neuripsxaiw.txt">Bibtex</a>
        <p>Training data attribution (TDA) provides a non-parametric viewpoint for model explanations - which training data points are blamable for this test error? Apparently useful in practice, we realised that the actual usefulness is not tested in real applications. As a first step, we approach individuals working in a diverse array of sectors, either using or developing ML models, and ask whether they would find TDA useful in practice. The answer is affirmative - read the paper for more details.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
          <img src="pictures/balint2023tml.png" alt="balint2023tml" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ff9aa2">Robustness</span>
        <span style="background-color:#ffdac1">Uncertainty</span>
        <span style="background-color:#ffffce">Human Annotation</span>
        <span style="background-color:#c7ceea">Explainability</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <span style="background-color:#C9D3D8">Large-Scale ML</span>
        <br>
        <a href="https://trustworthyml.io/" id="balint2023tml">
          <papertitle>Trustworthy Machine Learning
          </papertitle>
        </a>
        <br>
        <a href="https://scalabletrustworthyai.github.io/member/balint/">Bálint Mucsányi</a>,
        <a href="https://www.hci.uni-tuebingen.de/chair/team/michael-kirchhof">Michael Kirchhof</a>,
        <a href="https://elisanguyen.github.io/">Elisa Nguyen</a>,
        <a href="https://scalabletrustworthyai.github.io/member/alex/">Alexander Rubinstein</a>,
        <strong>Seong Joon Oh</strong>.
        <br>
        2023
        <br>
        <a href="data/balint2023tml.txt">Bibtex</a> /
        <a href="https://trustworthyml.io/">Webpage</a> /
        <a href="https://arxiv.org/abs/2310.08215">arXiv</a>
        <p>The challenges posed by the trustworthiness of machine learning models are increasingly significant as these models find real-world applications. Our newly-released textbook, "Trustworthy Machine Learning," aims to address these challenges comprehensively. It covers four crucial dimensions: Out-of-Distribution Generalization, Explainability, Uncertainty Quantification, and Evaluation of Trustworthiness. The text offers a thorough analysis of seminal and modern research papers, elucidating the foundational theories and practices. Originating from a course first offered at the University of Tübingen in the Winter Semester of 2022/23, the book serves as a stand-alone resource and includes code snippets and additional references. For further information, please visit our dedicated website.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
          <img src="pictures/elisa2023neurips.png" alt="elisa2023neurips" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ffdac1">Uncertainty</span>
        <span style="background-color:#c7ceea">Explainability</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <br>
        <a href="https://arxiv.org/abs/2305.19765" id="elisa2023neurips">
          <papertitle>A Bayesian Perspective On Training Data Attribution
          </papertitle>
        </a>
        <br>
        <a href="https://elisanguyen.github.io/">Elisa Nguyen</a>,
        <a href="https://seominjoon.github.io/">Minjoon Seo</a>,
        <strong>Seong Joon Oh</strong>.
        <br>
        <em>NeurIPS</em>, 2023
        <br>
        <a href="data/elisa2023neurips.txt">Bibtex</a> /
        <a href="https://github.com/ElisaNguyen/bayesian-tda">Code</a>
        <p>Consider <em>Training Data Attribution (TDA)</em> as a spotlight, highlighting the role each training sample plays in the predictions a model whips up. It's a tantalizing concept, especially for human-centric XAI, where it can guide users to tweak their training samples for better results. However, it's a bit like trying to hear a whisper in a storm. That's because the impact of removing a single training sample usually pales in comparison to the cacophony of noise stirred up during model training, like the random spark of model initialization or the chaotic dance of SGD batch shuffling. To understand this better, we've adopted a Bayesian deep learning viewpoint, treating our learned model as a Bayesian posterior and TDA estimates as random variables. Our findings? TDA is like trying to tune in to a radio station that's mostly static. It's really only effective in those rare instances when the impact of a single sample isn't lost in the noise. In those cases, TDA can indeed play a sweet tune!
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
          <img src="pictures/siwon2023neurips.png" alt="siwon2023neurips" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#b5ead7">Privacy & Security</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <span style="background-color:#C9D3D8">Large-Scale ML</span>
        <br>
        <a href="https://arxiv.org/abs/2307.01881" id="siwon2023neurips">
          <papertitle>ProPILE: Probing Privacy Leakage in Large Language Models
          </papertitle>
        </a>
        <br>
        <a href="https://sites.google.com/view/siwonkim">Siwon Kim</a>,
        <a href="https://sangdooyun.github.io/">Sangdoo Yun</a>,
        <a href="https://hwaranlee.github.io/">Hwaran Lee</a>,
        <a href="https://gubri.eu/">Martin Gubri</a>,
        <a href="http://data.snu.ac.kr/index.php/people/">Sungroh Yoon*</a>
        <strong>Seong Joon Oh*</strong>.
        <br>
        <em>*Equal contribution</em>
        <br>
        <em>NeurIPS Spotlight</em>, 2023
        <br>
        <a href="data/siwon2023neurips.txt">Bibtex</a>
        <p>Large language models (LLMs) are like giant sponges, soaking up vast amounts of data from the web. But amidst all that data, there could be some sensitive stuff, like personally identifiable information (PII). Makes you a bit worried, right? That's where our new tool, <em>ProPILE</em>, comes in. Think of it as a detective, helping people investigate if their personal data might be seeping out from these LLMs. You can create your own prompts based on your personal info to check how much of your PII are likely to be exposed to millions of users. ProPILE is one of our first efforts to empower data subjects to gain awareness and control over their own PII in the era of LLMs.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
          <img src="pictures/teney2023neurips.png" alt="teney2023neurips" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ff9aa2">Robustness</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <br>
        <a href="https://arxiv.org/abs/2209.00613" id="teney2023neurips">
          <papertitle>ID and OOD Performance Are Sometimes Inversely Correlated on Real-world Datasets
          </papertitle>
        </a>
        <br>
        <a href="https://www.damienteney.info/">Damien Teney</a>,
        <a href="https://linyongver.github.io/yonglin.github.io/">Lin Yong</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://ehsanabb.github.io/">Ehsan Abbasnejad</a>.
        <br>
        <em>NeurIPS Spotlight</em>, 2023
        <br>
        <a href="data/teney2023neurips.txt">Bibtex</a>
        <p>Several recent studies have reported positive correlations between in-distribution (ID) and out-of-distribution (OOD) generalisation performances.
        In particular, <a href="https://arxiv.org/abs/2207.09239">Wenzel et al. (2022)</a> found that <em>none</em> of the 31k networks examined on 172 dataset pairs has shown a trade-off, or a negative correlation, between the ID and OOD performances.
        They further recommend that, to improve the OOD generalisation, one can instead focus on improving the ID generalisation.
        <strong>We argue that this may not always be true</strong>.
        We present counterexamples where one does observe a trade-off between ID and OOD generalisation.
        We point to the selection method for networks as the key reason for the contradicting observations.
        We alter the recommendation to the field in a more nuanced manner.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
          <img src="pictures/kirchhof2023neuripsdb.png" alt="kirchhof2023neuripsdb" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ffdac1">Uncertainty</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <br>
        <a href="https://arxiv.org/abs/2307.03810" id="kirchhof2023neuripsdb">
          <papertitle>URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates
          </papertitle>
        </a>
        <br>
        <a href="https://www.hci.uni-tuebingen.de/chair/team/michael-kirchhof">Michael Kirchhof</a>,
        <a href="https://scholar.google.com/citations?user=NexA8EEAAAAJ&hl=hu">Bálint Mucsányi</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://www.edu.sot.tum.de/hctl/prof-dr-enkelejda-kasneci/">Enkelejda Kasneci</a>.
        <br>
        <em>NeurIPS Benchmarks and Datasets</em>, 2023
        <br>
        <a href="data/kirchhof2023neuripsdb.txt">Bibtex</a> /
        <a href="https://github.com/mkirchhof/url">Code</a>
        <p>NeurIPS D&B extension of the <a href="#kirchhof2023uaieai">UAI Epistemic AI Workshop paper</a> below.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
          <img src="pictures/kirchhof2023uaieai.png" alt="kirchhof2023uaieai" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ffdac1">Uncertainty</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <br>
        <a href="https://arxiv.org/abs/2307.03810" id="kirchhof2023uaieai">
          <papertitle>URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates
          </papertitle>
        </a>
        <br>
        <a href="https://www.hci.uni-tuebingen.de/chair/team/michael-kirchhof">Michael Kirchhof</a>,
        <a href="https://scholar.google.com/citations?user=NexA8EEAAAAJ&hl=hu">Bálint Mucsányi</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://www.edu.sot.tum.de/hctl/prof-dr-enkelejda-kasneci/">Enkelejda Kasneci</a>.
        <br>
        <em>UAI Epistemic AI Workshop Best Student Paper</em>, 2023
        <br>
        <a href="data/kirchhof2023uaieai.txt">Bibtex</a> /
        <a href="https://github.com/mkirchhof/url">Code</a>
        <p>We developed the Uncertainty-aware Representation Learning (URL) benchmark in our research. This tool evaluates the reliability of uncertainty estimates from pretrained models on unseen datasets. Its implementation is simple, requiring only four lines of code. In our experiment, we applied URL to ten models trained on ImageNet. Then, we tested these models on eight different datasets. The results showed that achieving transferable uncertainty quantification remains a challenge. We invite the community to work on this novel problem!
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
          <img src="pictures/elif2023arxiv.png" alt="elif2023arxiv" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#e2c7e5">Evaluation</span>
        <span style="background-color:#C9D3D8">Large-Scale ML</span>
        <br>
        <a href="https://arxiv.org/abs/2305.16867" id="elif2023arxiv">
          <papertitle>Playing repeated games with Large Language Models
          </papertitle>
        </a>
        <br>
        <a href="https://github.com/eliaka">Elif Akata</a>,
        <a href="https://sites.google.com/view/lionschulz/home">Lion Schulz</a>,
        <a href="https://www.kyb.tuebingen.mpg.de/person/124121/249684">Julian Coda-Forno</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://bethgelab.org/">Matthias Bethge</a>,
        <a href="https://www.kyb.tuebingen.mpg.de/person/103915/2549">Eric Schulz</a>.
        <br>
        <em>arXiv</em>, 2023
        <br>
        <a href="data/elif2023arxiv.txt">Bibtex</a>
        <p>Imagine Large Language Models (LLMs) as digital diplomats, interacting with us and others in the cyber world. We set LLMs - GPT-3, GPT-3.5, and GPT-4 - against each other in games to understand their social behavior. LLMs are great when self-interest rules, like in the Prisoner's Dilemma, but stumble when coordination is key. GPT-4, for instance, acts tough in the Prisoner's Dilemma and struggles with simple conventions in the Battle of the Sexes. But, give GPT-4 more info or ask it to predict the opponent's move, and it adjusts its strategy. Our insights open up an exciting path towards a behavioral game theory for machines!
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
          <img src="pictures/han2023iccv.png" alt="han2023iccv" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ff9aa2">Robustness</span>
        <span style="background-color:#ffdac1">Uncertainty</span>
        <span style="background-color:#ffffce">Human Annotation</span>
        <span style="background-color:#c7ceea">Explainability</span>
        <span style="background-color:#C9D3D8">Large-Scale ML</span>
        <br>
        <a href="https://arxiv.org/abs/2303.17595" id="han2023iccv">
          <papertitle>Neglected Free Lunch -- Learning Image Classifiers Using Annotation Byproducts
          </papertitle>
        </a>
        <br>
        <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han*</a>,
        <a href="https://sites.google.com/site/junsukchoe/">Junsuk Choe*</a>,
        <a href="https://github.com/1000ship">Dante Chun</a>,
        <a href="https://johnr0.github.io/">John Joon Young Chung</a>,
        <a href="https://minsukchang.com/">Minsuk Chang</a>,
        <a href="https://sangdooyun.github.io/">Sangdoo Yun</a>,
        <a href="https://jyskwon.github.io/">Jean Y. Song</a>,
        <strong>Seong Joon Oh</strong>.
        <br>
        <em>*Equal contribution</em>
        <br>
        <em>ICCV</em>, 2023
        <br>
        <a href="data/han2023iccv.txt">Bibtex</a> /
        <a href="https://github.com/naver-ai/NeglectedFreeLunch">Code</a> /
        <a href="https://www.youtube.com/watch?v=9HEj3Km2TWo">Youtube</a> /
        <a href="data/han2023iccv_poster.pdf">Poster</a> /
        <a href="https://huggingface.co/datasets/coallaoh/ImageNet-AB">ImageNet-AB (HuggingFace)</a> /
        <a href="https://huggingface.co/datasets/coallaoh/COCO-AB">COCO-AB (HuggingFace)</a>
        <br>
        <a href="https://github.com/naver-ai/imagenet-annotation-tool">ImageNet annotation tool</a> /
        <a href="https://github.com/naver-ai/coco-annotation-tool">COCO annotation tool</a>
        <p>Supervised learning trains models with (X,Y) data.
        The (X,Y) data comes from the annotation procedure where annotators provide the correct Y for each X. 
        But behind the scene, annotators generate much more data than the (X,Y) data themselves: they unintionally generate auxiliary information during the annotation task, such as the history of corrections and the time-series of mouse traces and clicks.
        We call them <em>annotation byproducts (AB)</em> Z.
        We propose the new paradigm of <em>learning using annotation byproducts (LUAB)</em>, where models are trained with the triplets (X,Y,Z) involving the ABs.
        We reproduce the original annotation procedures for ImageNet and COCO to generate AB-enriched datasets: ImageNet-AB and COCO-AB. we show that the auxiliary Z may help models be better aligned with human recognition mechanisms, leading to improved model robustness.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
          <img src="pictures/nam2023iccv.png" alt="nam2023iccv" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
      <span style="background-color:#C9D3D8">Large-Scale ML</span>
        <br>
        <a href="https://arxiv.org/abs/2210.08457" id="nam2023iccv">
          <papertitle>Scratching Visual Transformer's Back with Uniform Attention
          </papertitle>
        </a>
        <br>
        <a href="https://ami.postech.ac.kr/members/nam-hyeon-woo">Hyeon-Woo Nam</a>,
        <a href="https://ug-kim.notion.site/">Yu-Ji Kim</a>,
        <a href="https://sites.google.com/view/byeongho-heo/home">Byeongho Heo</a>,
        <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://ami.postech.ac.kr/members/tae-hyun-oh">Tae-Hyun Oh</a>.
        <br>
        <em>ICCV</em>, 2023
        <br>
        <a href="data/nam2023iccv.txt">Bibtex</a> /
        <a href="https://uniform-attention.github.io/">Code</a>
        <p>ViT’s itchy point seems to be the uniform attention.
          ViTs are hungry for denser connections, yet dense connections are hard to achieve because of softmax's steep gradient around the uniform attention.
          We manually insert additional uniform attention layers in ViT models.
          This is very cheap!
          It turns out to be an effective trick for increasing the capacity and generalisation for ViT models, especially for the smaller versions.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
          <img src="pictures/kirchhof2023icml.png" alt="kirchhof2023icml" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ffdac1">Uncertainty</span>
        <br>
        <a href="https://arxiv.org/abs/2302.02865" id="kirchhof2023icml">
          <papertitle>Probabilistic Contrastive Learning Recovers the Correct Aleatoric Uncertainty of Ambiguous Inputs
          </papertitle>
        </a>
        <br>
        <a href="https://www.hci.uni-tuebingen.de/chair/team/michael-kirchhof">Michael Kirchhof</a>,
        <a href="https://www.edu.sot.tum.de/hctl/prof-dr-enkelejda-kasneci/">Enkelejda Kasneci</a>,
        <strong>Seong Joon Oh</strong>.
        <br>
        <em>ICML</em>, 2023
        <br>
        <a href="data/kirchhof2023icml.txt">Bibtex</a> /
        <a href="https://github.com/mkirchhof/Probabilistic_Contrastive_Learning">Code</a>
        <p>We finally came up with some theoretical guarantees for <a href="#joon2019iclr">probabilistic embeddings</a>! 
          Given a spherical embedding space with a von-Mises-Fisher (vMF) family of true latent embedding distribution, one may identify the true latent vMF for every data point up to rotations with a Monte-Carlo version of InfoNCE (called <em>MCInfoNCE</em>).
          This result is a probabilistic extension of the work by <a href="https://brendel-group.github.io/cl-ica/">Zimmerman <em>et al.</em></a>
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
          <img src="pictures/hwang2022neurips.png" alt="hwang2022neurips" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ff9aa2">Robustness</span>
        <br>
        <a href="https://arxiv.org/abs/2211.02291" id="hwang2022neurips">
          <papertitle>SelecMix: Debiased Learning by Contradicting-pair Sampling
          </papertitle>
        </a>
        <br>
        <a href="https://bluemoon010.github.io/">Inwoo Hwang</a>,
        <a href="https://bi.snu.ac.kr/People/index.html">Sangjun Lee</a>,
        <a href="https://bi.snu.ac.kr/People/index.html">Yunhyeok Kwak</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://www.damienteney.info/">Damien Teney</a>,
        <a href="http://wityworks.com/">Jin-Hwa Kim</a>,
        <a href="https://bi.snu.ac.kr/~btzhang/">Byoung-Tak Zhang</a>.
        <br>
        <em>NeurIPS</em>, 2022
        <br>
        <a href="data/hwang2022neurips.txt">Bibtex</a> /
        <a href="data/hwang2022neurips_workshop.pdf">Workshop paper</a>
        <p>A classifier gets biased when its decision boundary separates the bias attribute (<em>e.g.</em> gender attribute for profession prediction).
          Some prior de-biasing methods correct the decision boundary by identifying the <em>bias-conflicting</em> samples in the training data (<em>e.g.</em> female mechanical engineers) and giving more weight on them.
          We go one step further.
          We argue that it's more effective to augment the whole convex hull between usual data points (<em>e.g.</em> male mechanical engineers) and <em>bias-conflicting</em> samples (<em>e.g.</em> female mechanical engineers).
          We do this through simple Mixup.
          It effectively de-biases a model, even in the presence of strong label noise, arguably the greatest arch-enemy for a de-biasing method.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
          <img src="pictures/chun2022eccv.png" alt="chun2022eccv" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ffffce">Human Annotation</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <span style="background-color:#C9D3D8">Large-Scale ML</span>
        <br>
        <a href="https://arxiv.org/abs/2204.03359" id="chun2022eccv">
          <papertitle>ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO
          </papertitle>
        </a>
        <br>
        <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>,
        <a href="https://wonjae.kim/">Wonjae Kim</a>,
        <a href="https://8uos.github.io/">Song Park</a>,
        <a href="https://minsukchang.com/">Minsuk Chang</a>,
        <strong>Seong Joon Oh</strong>.
        <br>
        <em>ECCV</em>, 2022
        <br>
        <a href="data/chun2022eccv.txt">Bibtex</a> /
        <a href="https://github.com/naver-ai/eccv-caption">Code</a> /
        <a href="https://docs.google.com/presentation/d/1OKaWPlNblepiXF57oWs2miGgYb5kuu1qxNqV_-hDddU/edit?usp=sharing">Slides (long)</a> /
        <a href="https://docs.google.com/presentation/d/1zyLL49_2-F6mQFaMIumPfdE7el_r048XtidLnehepHo/edit?usp=sharing">Slides (short)</a>
        <p>Image-captioning benchmarks such as <a href="https://paperswithcode.com/dataset/coco-captions">COCO Captions</a> contain lots of nonsense.
          For the same image on the left, the caption that goes "Playing tennis with a racket" is deemed correct, while "Swinging a tennis racket" is penalised.
          This comes from the erratic recipe for constructing the datasets: (1) let annotators write down 5 captions per image and (2) consider <em>only</em> those 5 captions to be correct matches.
          We show that this practice introduces a lot of noise in the evaluation benchmarks.
          We then introduce <a href="https://github.com/naver-ai/eccv-caption">a novel image-captioning dataset</a> based on the MS-COCO Captions that captures the model performances more precisely.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/kim2022icml.png" alt="kim2022icml" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ffffce">Human Annotation</span>
        <span style="background-color:#C9D3D8">Large-Scale ML</span>
        <br>
        <a href="https://arxiv.org/abs/2205.14959" id="kim2022icml">
          <papertitle>Dataset Condensation via Efficient Synthetic-Data Parameterization
          </papertitle>
        </a>
        <br>
        <a href="https://janghyun1230.github.io/">Jang-Hyun Kim</a>,
        <a href="https://mllab.snu.ac.kr/people.html">Jinuk Kim</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://sangdooyun.github.io/">Sangdoo Yun</a>,
        <a href="https://songhwanjun.github.io/">Hwanjun Song</a>,
        Joonhyun Jeong,
        <a href="https://aidljwha.wordpress.com/">Jung-Woo Ha</a>,
        <a href="https://mllab.snu.ac.kr/hyunoh/">Hyun Oh Song</a>.
        <br>
        <em>ICML</em>, 2022
        <br>
        <a href="data/kim2022icml.txt">Bibtex</a> /
        <a href="https://github.com/snu-mllab/Efficient-Dataset-Condensation">Code</a>
        <p>Dataset condensation is the art of compactifying a training dataset.
        The aim is that a model trained on a condensed dataset is <em>similar</em> to the one trained on the original dataset, most importantly in terms of model accuracy (<em>e.g.</em> <a href="https://groups.inf.ed.ac.uk/vico/research/DatasetCondensation/">91%-accuracy MNIST classifier with only 1 sample per class</a>).
        We introduce many practical tricks to make data condensation work beyond the toy setting.
        We present the first data condensation method that <em>actually</em> works on images with sizes as large as 224x224, instead of 32x32!
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/lee2022cvpr.png" alt="lee2022cvpr" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ff9aa2">Robustness</span>
        <span style="background-color:#ffffce">Human Annotation</span>
        <span style="background-color:#c7ceea">Explainability</span>
        <br>
        <a href="https://arxiv.org/abs/2203.03860" id="lee2022cvpr">
          <papertitle>Weakly Supervised Semantic Segmentation Using Out-of-Distribution Data
          </papertitle>
        </a>
        <br>
        <a href="https://github.com/jbeomlee93">Jungbeom Lee</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://sangdooyun.github.io/">Sangdoo Yun</a>,
        <a href="https://sites.google.com/site/junsukchoe/">Junsuk Choe</a>,
        <a href="http://data.snu.ac.kr/index.php/people/">Eunji Kim</a>,
        <a href="http://data.snu.ac.kr/index.php/people/">Sungroh Yoon</a>.
        <br>
        <em>CVPR</em>, 2022
        <br>
        <a href="data/lee2022cvpr.txt">Bibtex</a> /
        <a href="https://github.com/naver-ai/w-ood">Code</a>
        <p>Weakly-supervised semantic segmentation (WSSS) is the task of solving pixel-wise class assignment with only the image-level supervision.
          The problem is <a href="#choe2020cvpr">ill-posed</a> because the image-level labels alone do not let models distinguish foreground (FG) objects (<em>e.g.</em> train) from spuriously-correlated background (BG) cues (<em>e.g.</em> rail).
          Researchers have sought <a href="https://www.youtube.com/watch?v=jM1T1HwbY5s">external sources of information</a>, such as shape prior, to address the ill-posedness.
          In this paper, we explore a novel source: <em>BG images</em> (<em>e.g.</em> rail images without a train).
          Conceptually, telling models what are <em>not</em> the FG cues is equivalent to telling them what <em>actually</em> are the FG cues; BG images are sufficient for turning the problem into a well-posed one.
          Collecting such BG data is cost-efficient, requiring orders of magnitude less annotation costs than the already-cheap image-level labels.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/scimeca2022iclr.png" alt="scimeca2022iclr" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ff9aa2">Robustness</span>
        <span style="background-color:#c7ceea">Explainability</span>
        <br>
        <a href="https://arxiv.org/abs/2110.03095" id="scimeca2022iclr">
          <papertitle>Which Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space Perspective
          </papertitle>
        </a>
        <br>
        <a href="https://lucascimeca.com/">Luca Scimeca*</a>,
        <strong>Seong Joon Oh*</strong>,
        <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>,
        <a href="https://zymrael.github.io/">Michael Poli</a>,
        <a href="https://sangdooyun.github.io/">Sangdoo Yun</a>.
        <br>
        <em>*Equal contribution</em>
        <br>
        <em>ICLR</em>, 2022
        <br>
        <a href="data/scimeca2022iclr.txt">Bibtex</a>
        <p><a href="https://arxiv.org/abs/2004.07780">Shortcut learning</a> is emerging as a key limitation of the current generation of machine learning models (<a href="#choe2020cvpr">CVPR'20</a>, <a href="#hyojin2020icml">ICML'20</a>).
          In this work, instead of proposing yet another solution, we take a step back and deepen our understanding of the problem.
          For example, trained on a dataset where both colour and shape are valid cues for recognising the object,
          models of different types (MLP, CNN, and ViT) choose to use colour over shape.
          Why is that? We provide an explanation from the parameter-space perspective. Read the paper. Worth it!
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/hazel2022aaai.png" alt="hazel2022aaai" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ff9aa2">Robustness</span>
        <span style="background-color:#ffffce">Human Annotation</span>
        <br>
        <a href="https://arxiv.org/abs/2112.11916" id="hazel2022aaai">
          <papertitle>ALP: Data Augmentation using Lexicalized PCFGs for Few-Shot Text Classification
          </papertitle>
        </a>
        <br>
        <a href="https://github.com/hazelhkim">Hazel Kim</a>,
        Daecheol Woo,
        <strong>Seong Joon Oh</strong>,
        <a href="https://www.linkedin.com/in/jeong-won-cha-a936142a/">Jeong-Won Cha</a>,
        <a href="http://toc.yonsei.ac.kr/~emmous/">Yo-Sub Han</a>.
        <br>
        <em>AAAI</em>, 2022
        <br>
        <a href="data/hazel2022aaai.txt">Bibtex</a>
        <p>This is an NLP paper. There have been many attempts at enlarging the training text data for few-shot text classification, like back-translation (<em>e.g.</em> En-Fr-En) and the use of pre-trained language models.
          Unlike those, we propose an augmentation method that is fully aware of the underlying grammatical structure of the sentence.
          Importantly, our method generates a set of synonymous sentences that are both grammatically correct and grammatically diverse!
          Here we gain quite some points in few-shot text classification benchmarks.
          Another contribution is viewing the train-val split <em>as part of the method</em> and seeking the best splitting strategy when data augmentation is being used.
          It turns out that splitting the few-shot labelled samples <em>S</em> into disjoint train-val splits (train split is then augmented) is sub-optimal; a better strategy is to use the augmented source data <em>S'</em> as the train split and the original <em>S</em> itself as the validation split!
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/choe2020cvpr.gif" alt="choe2020cvpr" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ff9aa2">Robustness</span>
        <span style="background-color:#ffffce">Human Annotation</span>
        <span style="background-color:#c7ceea">Explainability</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <br>
        <a href="https://arxiv.org/abs/2007.04178" id="choe2022tpami">
          <papertitle>Evaluation for Weakly Supervised Object Localization: Protocol, Metrics, and Datasets
          </papertitle>
        </a>
        <br>
        <a href="https://sites.google.com/site/junsukchoe/">Junsuk Choe*</a>,
        <strong>Seong Joon Oh*</strong>,
        <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>,
        <a href="https://eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>,
        <a href="https://sites.google.com/site/katehyunjungshim/home">Hyunjung Shim</a>.
        <br>
        <em>*Equal contribution</em>
        <br>
        <em>TPAMI</em>, 2022
        <br>
        <a href="data/choe2022tpami.txt">Bibtex</a> /
        <a href="https://github.com/clovaai/wsolevaluation">Code</a> /
        <a href="data/choe2020cvpr_slides.pdf">Slides</a> /
        <a href="https://www.youtube.com/watch?v=D_dEkeb-fto&list=PLcD_yLvcdUll95mAnBDV0rZKhfClJMZMr&index=5">Tutorial video</a>
        <p>Journal extension of <a href="#choe2020cvpr">CVPR'20</a>! It now contains more analyses, including the evaluation of input gradient variants as Weakly-Supervised Object Localization (WSOL) methods.</p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/kim2021iccv.png" alt="kim2021iccv" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ff9aa2">Robustness</span>
        <span style="background-color:#ffffce">Human Annotation</span>
        <span style="background-color:#c7ceea">Explainability</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <br>
        <a href="https://arxiv.org/abs/2106.07861" id="kim2021iccv">
          <papertitle>Keep CALM and Improve Visual Feature Attribution
          </papertitle>
        </a>
        <br>
        <a href="https://jaemyung-kim.github.io/">Jae Myung Kim*</a>,
        <a href="https://sites.google.com/site/junsukchoe/">Junsuk Choe*</a>,
        <a href="https://eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>,
        <strong>Seong Joon Oh</strong>.
        <br>
        <em>*Equal contribution</em>
        <br>
        <em>ICCV</em>, 2021
        <br>
        <a href="data/kim2021iccv.txt">Bibtex</a> /
        <a href="https://github.com/naver-ai/calm">Code</a>
        <p>It is difficult to find a CV researcher or practitioner who hasn't used (or at least heard of) the Class Activation Maps (<a href="http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf">CAM</a>).
          It is a seminal  <em>feature attribution</em> method that has left a deep mark on the vision research and applications.
          Notwithstanding its popularity, we found some practical and conceptual issues that makes CAM not as interpretable as it should be.
          We address the issues with a probabilistic treatment of the last layers of CNNs where the latent <em>cue</em> variable <em>Z</em> is trained via Marginal Likelihood (ML) or Expectation-Maximisation (EM) algorithms.
          The resulting <em>Class Activation Latent Maps</em>, or <em>CALM</em>, produces more precise and interpretable score maps.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/heo2021iccv.png" alt="heo2021iccv" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#C9D3D8">Large-Scale ML</span>
        <br>
        <a href="https://arxiv.org/abs/2103.16302" id="heo2021iccv">
          <papertitle>Rethinking Spatial Dimensions of Vision Transformers
          </papertitle>
        </a>
        <br>
        <a href="https://sites.google.com/view/byeongho-heo/home">Byeongho Heo</a>,
        <a href="https://sangdooyun.github.io/">Sangdoo Yun</a>,
        <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>,
        <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>,
        <a href="https://sites.google.com/site/junsukchoe/">Junsuk Choe</a>,
        <strong>Seong Joon Oh</strong>.
        <br>
        <em>ICCV</em>, 2021
        <br>
        <a href="data/heo2021iccv.txt">Bibtex</a> /
        <a href="https://github.com/naver-ai/pit">Code</a>
        <p>The <a href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Tranformer</a> architecture has successfully been adapted to visual models (<em>e.g.</em> <a href="https://openreview.net/forum?id=YicbFdNTTy">ViT</a>).
          However, Transformers, originally designed for language modelling, and ViT assign a constant ratio of computational loads between spatial and channel dimensions at different depths.
          We postulate this as a suboptimal design choice, as CNNs assign different ratios at different depths to maximise the utility of compute.
          We thus present <em>Pooling-based Vision Transformer (PiT)</em> that does this.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/poli2021neurips.png" alt="poli2021neurips" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
      <span style="background-color:#ff9aa2">Robustness</span>
        <br>
        <a href="https://arxiv.org/abs/2106.04165" id="poli2021neurips">
          <papertitle>Neural Hybrid Automata: Learning Dynamics with Multiple Modes and Stochastic Transitions
          </papertitle>
        </a>
        <br>
        <a href="https://zymrael.github.io/">Michael Poli*</a>,
        <a href="https://github.com/massastrello">Stefano Massaroli*</a>,
        <a href="https://lucascimeca.com/">Luca Scimeca</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>,
        <a href="http://www.robot.t.u-tokyo.ac.jp/~yamashita/">Atsushi Yamashita</a>,
        <a href="http://www.robot.t.u-tokyo.ac.jp/asamalab/Asama_page/top_a_e.html">Hajime Asama</a>,
        <a href="http://silab.kaist.ac.kr/our-team/">Jinkyoo Park</a>,
        <a href="https://animesh.garg.tech/">Animesh Garg</a>.
        <br>
        <em>*Equal contribution</em>
        <br>
        <em>NeurIPS</em>, 2021
        <br>
        <a href="data/poli2021neurips.txt">Bibtex</a>
        <p>Recovering the dynamical systems, or the data generation process, behind time series data enables an effective and robust prediction, interpretation, and forecasting.
          There exist prior methods for recovering either continuous or discrete dynamics, but not the mixture.
          The underlying dynamics behind many real-world systems contain both continuous and discrete elements.
          For example, an aircraft essentially follows a continuous dynamics but goes through a discrete mode shift at touchdown.
          Such a system is referred to as a <a href="https://www.springer.com/gp/book/9783540334668">Stochastic Hybrid System (SHS)</a>.
          We present a framework that recovers SHS from time series data using ingredients like <a href="https://papers.nips.cc/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf">Neural ODEs</a> and <a href="https://en.wikipedia.org/wiki/Latent_variable_model">latent variable models</a>.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/yun2021cvpr.png" alt="yun2021cvpr" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ff9aa2">Robustness</span>
        <span style="background-color:#C9D3D8">Large-Scale ML</span>
        <br>
        <a href="https://arxiv.org/abs/2101.05022" id="yun2021cvpr">
          <papertitle>Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels
          </papertitle>
        </a>
        <br>
        <a href="https://sangdooyun.github.io/">Sangdoo Yun</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://sites.google.com/view/byeongho-heo/home">Byeongho Heo</a>,
        <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>,
        <a href="https://sites.google.com/site/junsukchoe/">Junsuk Choe</a>,
        <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>.
        <br>
        <em>CVPR</em>, 2021
        <br>
        <a href="data/yun2021cvpr.txt">Bibtex</a> /
        <a href="https://github.com/naver-ai/relabel_imagenet">Code</a>
        <p>ImageNet labels contain lots of noise (<em>e.g.</em> <a href="http://proceedings.mlr.press/v119/shankar20c.html">Shankar <em>et al.</em></a>).
        There have been efforts to fix them on the evaluation set, but not yet on the training set.
        We fix them on the training set (published at <a href="https://github.com/naver-ai/relabel_imagenet">codebase</a>), but with the help of a bigger image classifier, to make the task feasible at all.
        This is another trick that will improve the ImageNet & downstream task accuracies across the board.</p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/chun2021cvpr.png" alt="chun2021cvpr" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
          <span style="background-color:#ffdac1">Uncertainty</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <br>
        <a href="https://arxiv.org/abs/2101.05068" id="chun2021cvpr">
          <papertitle>Probabilistic Embeddings for Cross-Modal Retrieval
          </papertitle>
        </a>
        <br>
        <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://europe.naverlabs.com/people_user/Rafael-Sampaio-De-Rezende/">Rafael Sampaio de Rezende</a>,
        <a href="https://www.skamalas.com/">Yannis Kalantidis</a>,
        <a href="https://europe.naverlabs.com/people_user/diane-larlus/">Diane Larlus</a>.
        <br>
        <em>CVPR</em>, 2021
        <br>
        <a href="data/chun2021cvpr.txt">Bibtex</a> /
        <a href="https://github.com/naver-ai/pcme">Code</a>
        <p>Given an image, there are many ways to describe it in text. Given a text description, there are likewise many possible images that suits the description.
          Cross-model associations are of many-to-many nature. The usual deterministic embeddings cannot model this well.
          We introduce a probabilistic embedding scheme based on the <em>Hedged Instance Embedding</em> (<a href="#joon2019iclr">ICLR'19</a>) to handle the many-to-many mapping gracefully.
          We address another crucial issue with evaluation: your method gets either penalised or rewarded for retrieving synonymous sentences.
          This is because of the non-exhaustive true matches in the eval set.
          Since ground-up collection of such matches is too expensive, we introduce a novel surrogate measure <em>Plausible-Match R-Precision</em> based on the estimated true matches.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/heo2021iclr.png" alt="heo2021iclr" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#C9D3D8">Large-Scale ML</span>
        <br>
        <a href="https://arxiv.org/abs/2006.08217" id="heo2021iclr">
          <papertitle>AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights
          </papertitle>
        </a>
        <br>
        <a href="https://sites.google.com/view/byeongho-heo/home">Byeongho Heo*</a>,
        <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun*</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>,
        <a href="https://sites.google.com/site/youngjunguh">Youngjung Uh</a>,
        <a href="https://sangdooyun.github.io/">Sangdoo Yun</a>,
        <a href="https://sites.google.com/site/hjw9096/">Jungwoo Ha</a>.
        <br>
        <em>*Equal contribution</em>
        <br>
        <em>ICLR</em>, 2021
        <br>
        <a href="data/heo2021iclr.txt">Bibtex</a> /
        <a href="https://github.com/clovaai/AdamP">Code</a> /
        <a href="https://clovaai.github.io/AdamP/">Project</a>
        <p>When you apply a momentum-based optimizer over scale-invariant parameters, their norms increase quite a bit.
          The norm increase doesn't contribute anything to the loss minimization while only slowing down the convergence.
          We fix this by appending a projection operation on SGD and Adam. This leads to performance improvements across the board. </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/yun2020videomix.png" alt="yun2020videomix" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#C9D3D8">Large-Scale ML</span>
        <br>
        <a href="https://arxiv.org/abs/2012.03457" id="yun2020videomix">
          <papertitle>VideoMix: Rethinking Data Augmentation for Video Classification
          </papertitle>
        </a>
        <br>
        <a href="https://sangdooyun.github.io/">Sangdoo Yun</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://sites.google.com/view/byeongho-heo/home">Byeongho Heo</a>,
        <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>,
        Jinhyung Kim.
        <br>
        <em>arXiv</em>, 2020
        <br>
        <a href="data/yun2020videomix.txt">Bibtex</a>
        <p>Data augmentation is not as extensively studied in the video recognition tasks as in the static image recognition domain.
        We study the extension of popular static-image augmentation method, such as CutMix, on video recognition tasks.</p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/ferjad2020icml.png" alt="ferjad2020icml" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#e2c7e5">Evaluation</span>
        <br>
        <a href="https://arxiv.org/abs/2002.09797" id="ferjad2020icml">
          <papertitle>Reliable Fidelity and Diversity Metrics for Generative Models
          </papertitle>
        </a>
        <br>
        <a href="https://ferjad.github.io/">Muhammad Ferjad Naeem*</a>,
        <strong>Seong Joon Oh*</strong>,
        <a href="https://sites.google.com/site/youngjunguh">Youngjung Uh</a>,
        <a href="https://yunjey.github.io/">Yunjey Choi</a>,
        <a href="https://people.epfl.ch/jaejun.yoo/?lang=en">Jaejun Yoo</a>.
        <br>
        <em>*Equal contribution</em>
        <br>
        <em>ICML</em>, 2020
        <br>
        <a href="data/ferjad2020icml.txt">Bibtex</a> /
        <a href="https://github.com/clovaai/generative-evaluation-prdc">Code</a> /
        <a href="https://icml.cc/virtual/2020/poster/5832">ICML Virtual</a> /
        <a href="https://www.youtube.com/watch?v=_XwsGkryVpk&feature=youtu.be&ab_channel=FerjadNaeem">Youtube</a>
        <p>Evaluating generative models is tricky. There are Inception Score and Fr&eacute;chet Inception Distance measures indeed, and then (Improved) Precision and Recall metrics to separately examine the fidelity and diversity aspects.
        Yet, they are still not perfect. We address the issues with Improved Precision and Recall metrics and propose new metrics: Density and Coverage.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/bahng2020icml.png" alt="bahng2020icml" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ff9aa2">Robustness</span>
        <span style="background-color:#ffffce">Human Annotation</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <br>
        <a href="https://arxiv.org/abs/1910.02806" id="hyojin2020icml">
          <papertitle>Learning De-biased Representations with Biased Representations
          </papertitle>
        </a>
        <br>
        <a href="https://hjbahng.github.io/">Hyojin Bahng</a>,
        <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>,
        <a href="https://sangdooyun.github.io/">Sangdoo Yun</a>,
        <a href="https://sites.google.com/site/jaegulchoo/">Jaegul Choo</a>,
        <strong>Seong Joon Oh</strong>.
        <br>
        <em>ICML</em>, 2020
        <br>
        <a href="data/hyojin2020icml.txt">Bibtex</a> /
        <a href="https://github.com/clovaai/rebias">Code</a> /
        <a href="https://icml.cc/virtual/2020/poster/5783">ICML Virtual</a> /
        <a href="https://www.youtube.com/watch?v=lkjMxZDGubA">Youtube</a>
        <p>Models pick up correlations, rather than causal mechanisms, between inputs and outputs.
        De-biasing (and fairness) researches have guided models on "which cues to look at" through explicit bias labels or by re-weighting or re-generating training data to remove bias.
        We show that, for many application scenarios, it is possible to encode the "cues to look at" through model architecture and such expensive strategies are no longer needed.  </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/choe2020cvpr.gif" alt="choe2020cvpr" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ff9aa2">Robustness</span>
        <span style="background-color:#ffffce">Human Annotation</span>
        <span style="background-color:#c7ceea">Explainability</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <br>
        <a href="https://arxiv.org/abs/2001.07437" id="choe2020cvpr">
          <papertitle>Evaluating Weakly-Supervised Object Localization Methods Right
          </papertitle>
        </a>
        <br>
        <a href="https://sites.google.com/site/junsukchoe/">Junsuk Choe*</a>,
        <strong>Seong Joon Oh*</strong>,
        <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>,
        <a href="https://eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>,
        <a href="https://sites.google.com/site/katehyunjungshim/home">Hyunjung Shim</a>.
        <br>
        <em>CVPR</em>, 2020
        <br>
        <a href="data/choe2020cvpr.txt">Bibtex</a> /
        <a href="https://github.com/clovaai/wsolevaluation">Code</a> /
        <a href="data/choe2020cvpr_slides.pdf">Slides</a> /
        <a href="https://www.youtube.com/watch?v=D_dEkeb-fto&list=PLcD_yLvcdUll95mAnBDV0rZKhfClJMZMr&index=5">Tutorial video</a>
        <p>I have long waited for this moment since <a href="#joon2017cvpr">CVPR'17</a>. Weakly-Supervised Object Localization, or WSOL, has in fact been not weakly supervised in a strict sense.
        Design choices and hyperparameters are validated with the localization annotations!
        This paper explains <em>why</em> researchers <em>had to</em> rely on localization validation -- without localization supervision, there is no way to force a model to not extract cues from background regions.
        We propose a new fair benchmark acknowledging the need for localization annotations and show that WSOL methods since CAM in 2016 have not introduced much gain.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/lee2020cvprw.png" alt="lee2020cvprw" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <a href="https://arxiv.org/abs/1910.04396" id="lee2019cvprw">
          <papertitle>On Recognizing Texts of Arbitrary Shapes with 2D Self-Attention
          </papertitle>
        </a>
        <br>
        <a href="https://www.linkedin.com/in/junyeop-lee-7489aab7/">Junyeop Lee</a>,
        <a href="https://sites.google.com/view/sungraepark">Sungrae Park</a>,
        <a href="http://jeonghunbaek.net/">Jeonghun Baek</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://rosinality.github.io/">Seonghyeon Kim</a>,
        <a href="https://github.com/hwalsuklee">Hwalsuk Lee</a>.
        <br>
        <em>CVPR Workshop</em>, 2020
        <br>
        <a href="data/lee2020cvprw.txt">Bibtex</a>
        <p>Scene text recognition works well, but there are remaining corner cases. An example is texts with unusual orientations and arrangements (<em>e.g.</em> BMW logo).
        We focus on this corner case and propose a model based on self-attention. </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/joon2015iccv.png" alt="joon2015iccv" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#b5ead7">Privacy & Security</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <br>
        <a href="https://arxiv.org/abs/1710.03224" id="oh2018tpami">
          <papertitle>Person Recognition in Personal Photo Collections
          </papertitle>
        </a>
        <br>
        <strong>Seong Joon Oh</strong>,
        <a href="https://research.google/people/RodrigoBenenson/">Rodrigo Benenson</a>,
        <a href="https://cispa.saarland/group/fritz/">Mario Fritz</a>,
        <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>.
        <br>
        <em>TPAMI</em>, 2020
        <br>
        <a href="data/joon2020tpami.txt">Bibtex</a> /
        <a href="https://ieeexplore.ieee.org/document/8519337">Journal</a>
        <p>Journal version of my first paper <a href="#joon2015iccv">ICCV'15</a>, after five years! We have developed the version two of the <a href="#joon2015iccv">ICCV'15</a> system that outperforms the methods that have appeared in the meantime.</p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/joon2018iclr.png" alt="joon2018iclr" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#b5ead7">Privacy & Security</span>
        <br>
        <a href="https://arxiv.org/abs/1711.01768" id="oh2018iclr">
          <papertitle>Towards Reverse-Engineering Black-Box Neural Networks
          </papertitle>
        </a>
        <br>
        <strong>Seong Joon Oh</strong>,
        <a href="https://cispa.saarland/group/fritz/">Mario Fritz</a>,
        <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>.
        <br>
        <em>Explainable AI: Interpreting, Explaining and Visualizing Deep Learning (book chapter)</em>, 2019
        <br>
        <a href="data/joon2019blackboxchapter.txt">Bibtex</a> /
        <a href="https://link.springer.com/chapter/10.1007/978-3-030-28954-6_7">Book chapter</a>
        <p>Book chapter version of <a href="#joon2018iclr">ICLR'18</a>! We build connections between our black-box inspection methodology and the explainable AI.</p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/orekondy2019neuripsfl.png" alt="orekondy2019neuripsfl" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#b5ead7">Privacy & Security</span>
        <br>
        <a href="https://arxiv.org/abs/1805.05838" id="orekondy2019neuripsfl">
          <papertitle>Gradient-Leaks: Understanding and Controlling Deanonymization in Federated Learning
          </papertitle>
        </a>
        <br>
        <a href="https://tribhuvanesh.github.io/">Tribhuvanesh Orekondy</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://yangzhangalmo.github.io/">Yang Zhang</a>,
        <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>,
        <a href="https://cispa.saarland/group/fritz/">Mario Fritz</a>.
        <br>
        <em>NeurIPS Workshop</em>, 2019
        <br>
        <a href="data/orekondy19neuripsfl.txt">Bibtex</a> /
        <a href="data/orekondy2019neuripsfl_poster.pdf">Poster</a>
        <p>Federated learning allows sensitive user data to never leave the device and still be used for training. It is considered a safer option than sending the user data directly to the server.
        But is it? We show that users may be identified and linked based on the model updates communicated between the device and server.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/chun2019icmlw.png" alt="chun2019icmlw" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ff9aa2">Robustness</span>
        <span style="background-color:#ffdac1">Uncertainty</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <br>
        <a href="https://arxiv.org/abs/2003.03879" id="chun2019icmlw">
          <papertitle>An Empirical Evaluation on Robustness and Uncertainty of Regularization Methods
          </papertitle>
        </a>
        <br>
        <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://sangdooyun.github.io/">Sangdoo Yun</a>,
        <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>,
        <a href="https://sites.google.com/site/junsukchoe/">Junsuk Choe</a>,
        <a href="https://yjyoo3312.github.io/">Youngjoon Yoo</a>.
        <br>
        <em>ICML Workshop</em>, 2019
        <br>
        <a href="data/chun2019icmlw.txt">Bibtex</a>
        <p>There has been a line of research on simple regularization techniques like CutMix (<a href="#yun2019iccv">ICCV'19</a>) and other lines of research on robustness and uncertainty.
        We make a happy marriage of the two and measure how well the regularization techniques improve robustness and uncertainty of a model.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/yun2019iccv.png" alt="yun2019iccv" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ff9aa2">Robustness</span>
        <span style="background-color:#C9D3D8">Large-Scale ML</span>
        <br>
        <a href="https://arxiv.org/abs/1905.04899" id="yun2019iccv">
          <papertitle>CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features
          </papertitle>
        </a>
        <br>
        <a href="https://sangdooyun.github.io/">Sangdoo Yun</a>,
        <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>,
        <a href="https://sites.google.com/site/junsukchoe/">Junsuk Choe</a>,
        <a href="https://yjyoo3312.github.io/">Youngjoon Yoo</a>.
        <br>
        <em>ICCV Oral Talk</em>, 2019
        <br>
        <a href="data/yun2019cutmix.txt">Bibtex</a> /
        <a href="https://github.com/clovaai/CutMix-PyTorch">Code</a> /
        <a href="data/yun2019iccv_talk.pdf">Talk</a> /
        <a href="data/yun2019iccv_poster.pdf">Poster</a> /
        <a href="https://clova-ai.blog/2019/07/15/cutmix-regularization-strategy-to-train-strong-classifiers-with-localizable-features/">Blog</a> /
        <a href="https://clovaai.github.io/AdamP/">Project</a> /
        <a href="https://clovaai.github.io/AdamP/">Project</a>
        <p>A simple solution that works surprisingly well! Cut and paste patches from other images during training. Quite likely, you will see a performance boost.</p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/baek2019iccv.png" alt="baek2019iccv" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#e2c7e5">Evaluation</span>
        <br>
        <a href="https://arxiv.org/abs/1904.01906" id="baek2019iccv">
          <papertitle>What Is Wrong with Scene Text Recognition Model Comparisons? Dataset and Model Analysis
          </papertitle>
        </a>
        <br>
        <a href="http://jeonghunbaek.net/">Jeonghun Baek</a>,
        <a href="http://geewook.kim/">Geewook Kim</a>,
        <a href="https://www.linkedin.com/in/junyeop-lee-7489aab7/">Junyeop Lee</a>,
        <a href="https://sites.google.com/view/sungraepark">Sungrae Park</a>,
        <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>,
        <a href="https://sangdooyun.github.io/">Sangdoo Yun</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://github.com/hwalsuklee">Hwalsuk Lee</a>.
        <br>
        <em>ICCV Oral Talk</em>, 2019
        <br>
        <a href="data/baek2019STRcomparisons.txt">Bibtex</a> /
        <a href="https://github.com/clovaai/deep-text-recognition-benchmark">Code</a>
        <p>Scene text recognition field has long suffered from the lack of a unified agreement on the evaluation protocol.
        We provide a standard protocol. We also provide a unified view on the previous methods and discover a novel combination of existing modules that turns out to be the state of the art.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/joon2019iclr.png" alt="joon2019iclr" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ffdac1">Uncertainty</span>
        <br>
        <a href="https://arxiv.org/abs/1810.00319" id="joon2019iclr">
          <papertitle>Modeling Uncertainty with Hedged Instance Embedding
          </papertitle>
        </a>
        <br>
        <strong>Seong Joon Oh</strong>,
        <a href="https://research.google/people/KevinMurphy/">Kevin Murphy</a>,
        <a href="https://www.linkedin.com/in/jiyanpan/">Jiyan Pan</a>,
        <a href="https://research.google/people/JosephRoth/">Joseph Roth</a>,
        <a href="https://www.florian-schroff.de/">Florian Schroff</a>,
        <a href="https://research.google/people/AndrewGallagher/">Andrew Gallagher</a>.
        <br>
        <em>ICLR</em>, 2019
        <br>
        <a href="data/joon2019iclr.txt">Bibtex</a> /
        <a href="data/oh2019iclr_poster.pdf">Poster</a>
        <p>There has been quite some work on representing uncertainty for classification or regression tasks.
        Is there a way to represent uncertainty for instance embedding models too?
        We show that it is possible to train probabilistic representatitons for instances based on their inherent ambiguity.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/tretschk2018cscs.png" alt="tretschk2018cscs" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#b5ead7">Privacy & Security</span>
        <br>
        <a href="https://arxiv.org/abs/1805.12487" id="tretschk2018cscs">
          <papertitle>Sequential Attacks on Agents for Long-Term Adversarial Goals
          </papertitle>
        </a>
        <br>
        <a href="https://people.mpi-inf.mpg.de/~tretschk/">Edgar Tretschk</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://cispa.saarland/group/fritz/">Mario Fritz</a>.
        <br>
        <em>ACM CSCS</em>, 2018
        <br>
        <a href="data/edgar2018cscs.txt">Bibtex</a>
        <p>Can a bad guy hijack an RL agent? We show that it is possible to let an agent pursue an alternative reward by introducing small adversarial perturbations in the input stream.</p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/joon2018iclr.png" alt="joon2018iclr" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#b5ead7">Privacy & Security</span>
        <br>
        <a href="https://arxiv.org/abs/1711.01768" id="joon2018iclr">
          <papertitle>Towards Reverse-Engineering Black-Box Neural Networks
          </papertitle>
        </a>
        <br>
        <strong>Seong Joon Oh</strong>,
        Max Augustin,
        <a href="https://cispa.saarland/group/fritz/">Mario Fritz</a>,
        <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>.
        <br>
        <em>ICLR</em>, 2018
        <br>
        <a href="data/joon2018iclr.txt">Bibtex</a> /
        <a href="data/oh2018iclr_abstract.pdf">Extended abstract</a> /
        <a href="data/oh2018iclr_poster.pdf">Poster</a> /
        <a href="https://github.com/coallaoh/WhitenBlackBox">Code</a>
        <p>Recipes for training a high-performance model are not cheap. Think about the GPU-and-research-scientist-and-engineer hours to find the right architectural components and optimizer hyperparameters.
        What if they can be stolen by examining the model responses to certain inputs? </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/sun2017cvpr.png" alt="sun2017cvpr" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#b5ead7">Privacy & Security</span>
        <br>
        <a href="https://arxiv.org/abs/1711.09001" id="sun2018cvpr">
          <papertitle>Natural and Effective Obfuscation by Head Inpainting
          </papertitle>
        </a>
        <br>
        <a href="https://qianrusun.com/">Qianru Sun</a>,
        <a href="http://charliememory.github.io/">Liqian Ma</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html">Luc Van Gool</a>,
        <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>,
        <a href="https://cispa.saarland/group/fritz/">Mario Fritz</a>.
        <br>
        <em>CVPR</em>, 2018
        <br>
        <a href="data/qianru2018cvpr.txt">Bibtex</a>
        <p>Adversarial perturbation solutions (<a href="#joon2017iccv">ICCV'17</a>) produce visually pleasant protections with high protection rates, but their effects may be confined to a handful of recognition systems.
        We propose another solution based on face inpainting that changes the face to a fictitious yet natural-looking identity. It is effective against a broader set of recognition systems. </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/joon2017cvprw.png" alt="joon2017cvprw" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#b5ead7">Privacy & Security</span>
        <br>
        <a href="data/oh2017cvprw.pdf" id="joon2017cvprw">
          <papertitle>From Understanding to Controlling Privacy against Automatic Person Recognition in Social Media
          </papertitle>
        </a>
        <br>
        <strong>Seong Joon Oh</strong>,
        <a href="https://cispa.saarland/group/fritz/">Mario Fritz</a>,
        <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>.
        <br>
        <em>CVPR Workshop</em>, 2017
        <br>
        <a href="data/joon2017cvprw.txt">Bibtex</a> /
        <a href="data/oh2017cvprw_poster.pdf">Poster</a>
        <p>We stop and look back on the visual privacy papers (<a href="#joon2015iccv">ICCV'15</a>, <a href="#joon2016eccv">ECCV'16</a>, <a href="#joon2017iccv">ICCV'17</a>).</p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/joon2017iccv.png" alt="joon2017iccv" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ff9aa2">Robustness</span>
        <span style="background-color:#b5ead7">Privacy & Security</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <br>
        <a href="https://arxiv.org/abs/1703.09471" id="joon2017iccv">
          <papertitle>Adversarial Image Perturbation for Privacy Protection -- A Game Theory Perspective
          </papertitle>
        </a>
        <br>
        <strong>Seong Joon Oh</strong>,
        <a href="https://cispa.saarland/group/fritz/">Mario Fritz</a>,
        <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>.
        <br>
        <em>ICCV</em>, 2017
        <br>
        <a href="data/joon2017iccv.txt">Bibtex</a> /
        <a href="data/oh2017iccv_poster.pdf">Poster</a> /
        <a href="https://github.com/coallaoh/AIP">Code</a>
        <p>If face blurring doesn't work (<a href="#joon2016eccv">ECCV'16</a>), how should we shield our personal photos online against recognition systems?
          We propose a solution based on adversarial perturbations and the game theoretic considerations for the evaluation therein.</p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/joon2017cvpr.png" alt="joon2017cvpr" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ffffce">Human Annotation</span>
        <span style="background-color:#c7ceea">Explainability</span>
        <br>
        <a href="https://arxiv.org/abs/1701.08261" id="joon2017cvpr">
          <papertitle>Exploiting Saliency for Object Segmentation from Image Level Labels
          </papertitle>
        </a>
        <br>
        <strong>Seong Joon Oh</strong>,
        <a href="https://research.google/people/RodrigoBenenson/">Rodrigo Benenson</a>,
        <a href="https://www.linkedin.com/in/anna-khoreva/">Anna Khoreva</a>,
        <a href="https://eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>,
        <a href="https://cispa.saarland/group/fritz/">Mario Fritz</a>,
        <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>.
        <br>
        <em>CVPR</em>, 2017
        <br>
        <a href="data/joon2017cvpr.txt">Bibtex</a> /
        <a href="data/oh2017cvpr_poster.pdf">Poster</a> /
        <a href="https://github.com/coallaoh/GuidedLabelling">Code</a>
        <p>There has been quite some work around training models for localization tasks (<em>e.g.</em> semantic segmentation) from the image tag supervision only.
          But is this fundamentally possible without relying on extensive validation with full localization annotations?
          We argue that certain priors are necessary at the very least to encode the extent of objects. Saliency, we argue, is a handy prior.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/anja2017cvpr.png" alt="anja2017cvpr" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <a href="https://arxiv.org/abs/1704.01518" id="rohrbach2017cvpr">
          <papertitle>Generating Descriptions with Grounded and Co-Referenced People
          </papertitle>
        </a>
        <br>
        <a href="https://anna-rohrbach.net/">Anna Rohrbach</a>,
        <a href="https://rohrbach.vision/">Marcus Rohrbach</a>,
        <a href="https://inf.ethz.ch/people/person-detail.MjYyNzgw.TGlzdC8zMDQsLTg3NDc3NjI0MQ==.html">Siyu Tang</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>.
        <br>
        <em>CVPR</em>, 2017
        <br>
        <a href="data/anja2017cvpr.txt">Bibtex</a>
        <p>We casually use pronouns to refer to others. For machines, however, referring to people with pronouns necessitates new types of data and training strategies to explicitly localize and link people across frames. We do that. </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/joon2016eccv.png" alt="joon2016eccv" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#b5ead7">Privacy & Security</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <br>
        <a href="https://arxiv.org/abs/1607.08438" id="joon2016eccv">
          <papertitle>Faceless Person Recognition; Privacy Implications in Social Media
          </papertitle>
        </a>
        <br>
        <strong>Seong Joon Oh</strong>,
        <a href="https://research.google/people/RodrigoBenenson/">Rodrigo Benenson</a>,
        <a href="https://cispa.saarland/group/fritz/">Mario Fritz</a>,
        <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>.
        <br>
        <em>ECCV</em>, 2016
        <br>
        <a href="data/joon2016eccv.txt">Bibtex</a> /
        <a href="data/oh2016eccv_poster.pdf">Poster</a> /
        <a href="data/oh2016eccvw.pdf">Extended abstract</a>
        <p>But can you still be recognized even with a blur on your face? Quite likely.</p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/aditya2016mobisys.png" alt="aditya2016mobisys" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#b5ead7">Privacy & Security</span>
        <br>
        <a href="data/aditya2016mobisys.pdf" id="aditya2016mobisys">
          <papertitle>I-pic: A Platform for Privacy-Compliant Image Capture
          </papertitle>
        </a>
        <br>
        <a href="https://www.bell-labs.com/usr/paarijaat.aditya">Paarijaat Aditya</a>,
        <a href="http://www.cse.iitd.ernet.in/~rijurekha/">Rijurekha Sen</a>,
        <a href="https://people.mpi-sws.org/~druschel/">Peter Druschel</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://research.google/people/RodrigoBenenson/">Rodrigo Benenson</a>,
        <a href="https://cispa.saarland/group/fritz/">Mario Fritz</a>,
        <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>,
        <a href="https://www.cs.umd.edu/people/bobby">Bobby Bhattacharjee</a>,
        <a href="https://www.urmc.rochester.edu/biostat/people/faculty/wu-tongtong.aspx"> Tong Tong Wu</a>.
        <br>
        <em>MobiSys</em>, 2016
        <br>
        <a href="data/aditya2016mobisys.txt">Bibtex</a> /
        <a href="http://ipic.mpi-sws.org/">Project</a>
        <p>You are a janitor at Taj Mahal. Against you will, sightseers take photos with your face in the background. How can you opt out of being present in someone else's photo? We present a mobile-system based solution.</p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/joon2015iccv.png" alt="joon2015iccv" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#b5ead7">Privacy & Security</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <br>
        <a href="https://arxiv.org/abs/1509.03502" id="joon2015iccv">
          <papertitle>Person Recognition in Personal Photo Collections
          </papertitle>
        </a>
        <br>
        <strong>Seong Joon Oh</strong>,
        <a href="https://research.google/people/RodrigoBenenson/">Rodrigo Benenson</a>,
        <a href="https://cispa.saarland/group/fritz/">Mario Fritz</a>,
        <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Bernt Schiele</a>.
        <br>
        <em>ICCV</em>, 2015
        <br>
        <a href="data/joon2015iccv.txt">Bibtex</a> /
        <a href="data/oh2015iccv_poster.pdf">Poster</a> /
        <a href="https://www.youtube.com/watch?v=F4Jh0f3xD0g">Video</a> /
        <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/people-detection-pose-estimation-and-tracking/person-recognition-in-personal-photo-collections/">Project</a>
        <p>How well does a CNN model recognize people in personal photos? Even when people don't look at cameras, CNN finds out who they are, based on the context (<em>e.g.</em> location and social connections).</p>
      </div>
    </div>
  </div>


  <div class="container">
    <div class="row section-heading-rows">
      <h3>Academic activities</h3>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="https://scalabletrustworthyai.github.io/img/compass.png" alt="freepik image" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ff9aa2">Robustness</span>
        <span style="background-color:#ffdac1">Uncertainty</span>
        <span style="background-color:#ffffce">Human Annotation</span>
        <span style="background-color:#c7ceea">Explainability</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <span style="background-color:#C9D3D8">Large-Scale ML</span>
        <br>
        <a id="tml2324">
          <papertitle>Trustworthy Machine Learning
          </papertitle>
        </a>
        <br>
        <strong>Seong Joon Oh</strong>,
        <a href="https://scalabletrustworthyai.github.io/member/arnas/">Arnas Uselis</a>,
        <a href="https://scalabletrustworthyai.github.io/member/balint/">Bálint Mucsányi</a>,
        <a href="https://scalabletrustworthyai.github.io/member/evgenii/">Evgenii Kortukov</a>.
        <br>
        <em>Winter 23/24 @ University of Tübingen</em>
        <br>
        <a href="https://scalabletrustworthyai.github.io/courses/tml_winter_2324/">Website</a> /
        <a href="https://trustworthyml.io/">Book</a>
        <p>
          As machine learning technology gets applied to actual products and solutions, new challenges have emerged. Models unexpectedly fail to generalise well to small changes in the distribution; some models are found to utilise sensitive features that could treat certain demographic user groups unfairly; models tend to be confident on novel types of data; models cannot communicate the rationale behind their decisions effectively with the end users like medical staff to maximise the human-machine synergies. Collectively, we face a trustworthiness issue with the current machine learning technology. A large fraction of the machine learning research nowadays is dedicated to expanding the frontier of Trustworthy Machine Learning (TML). The course covers a theoretical and technical background for key topics in TML. We conduct a critical review of important classical and contemporary research papers on related topics and provide hands-on practicals to implement TML techniques.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="https://scalabletrustworthyai.github.io/img/DALLE_TML.png" alt="freepik image" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ff9aa2">Robustness</span>
        <span style="background-color:#ffdac1">Uncertainty</span>
        <span style="background-color:#ffffce">Human Annotation</span>
        <span style="background-color:#c7ceea">Explainability</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <span style="background-color:#C9D3D8">Large-Scale ML</span>
        <br>
        <a id="tml2223">
          <papertitle>Trustworthy Machine Learning
          </papertitle>
        </a>
        <br>
        <strong>Seong Joon Oh</strong>,
        <a href="https://elisanguyen.github.io/">Elisa Nguyen</a>,
        <a href="https://scalabletrustworthyai.github.io/member/alex/">Alexander Rubinstein</a>,
        <a href="https://scalabletrustworthyai.github.io/member/elif/">Elif Akata</a>,
        <a href="https://www.eml-unitue.de/people/michael-kirchhof">Michael Kirchhof</a>.
        <br>
        <em>Winter 22/23 @ University of Tübingen</em>
        <br>
        <a href="https://scalabletrustworthyai.github.io/courses/tml_winter_2223/">Website</a> /
        <a href="https://trustworthyml.io/">Book</a>
        <p>
          As machine learning technology gets applied to actual products and solutions, new challenges have emerged. Models unexpectedly fail to generalise well to small changes in the distribution; some models are found to utilise sensitive features that could treat certain demographic user groups unfairly; models tend to be confident on novel types of data; models cannot communicate the rationale behind their decisions effectively with the end users like medical staff to maximise the human-machine synergies. Collectively, we face a trustworthiness issue with the current machine learning technology. A large fraction of the machine learning research nowadays is dedicated to expanding the frontier of Trustworthy Machine Learning (TML). The course covers a theoretical and technical background for key topics in TML. We conduct a critical review of important classical and contemporary research papers on related topics and provide hands-on practicals to implement TML techniques.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/workshop.png" alt="freepik image" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ff9aa2">Robustness</span>
        <span style="background-color:#ffffce">Human Annotation</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <span style="background-color:#C9D3D8">Large-Scale ML</span>
        <br>
        <a id="imagenet2021neurips">
          <papertitle>Workshop on ImageNet: Past, Present, and Future
          </papertitle>
        </a>
        <br>
        <a href="https://eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>,
        <a href="http://lucasb.eyer.be/">Lucas Beyer</a>,
        <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>,
        <a href="https://eml-unitue.de/people/almut-sophia-koepke">Almut Sophia Koepke</a>,
        <a href="https://europe.naverlabs.com/people_user/diane-larlus/">Diane Larlus</a>,
        <strong>Seong Joon Oh</strong>,
        <a href="https://europe.naverlabs.com/people_user/Rafael-Sampaio-De-Rezende/">Rafael Sampaio de Rezende</a>,
        <a href="https://sangdooyun.github.io/">Sangdoo Yun</a>,
        <a href="https://sites.google.com/site/xzhai89">‪Xiaohua Zhai‬</a>.
        <br>
        <em>NeurIPS</em>, 2021
        <br>
        <a href="https://sites.google.com/view/imagenet-workshop/">Website</a>
        <p>
          <a href="https://image-net.org/">ImageNet</a> symbolises the stellar achievements in ML and CV in the past decade.
          It has served as the go-to benchmark for model architectures and training techniques and as a common pre-training dataset for numerous downstream tasks.
          As of 2021, ImageNet is going through a creative destruction.
          As the SOTA models are saturating towards the upper bound of the benchmark, new versions of the benchmarks are being proposed (<emph>e.g.</emph> ImageNet-<a href="https://arxiv.org/abs/1907.07174">A</a>/<a href="https://github.com/hendrycks/robustness">C</a>/<a href="https://arxiv.org/abs/2104.12928">D</a>/<a href="https://github.com/zhmiao/OpenLongTailRecognition-OLTR">LT</a>/<a href="https://arxiv.org/abs/1907.07174">O</a>/<a href="https://github.com/hendrycks/robustness">P</a>/<a href="https://github.com/hendrycks/imagenet-r">R</a>), with more focus on the reliability of models.
          Emerging fields in CV are now venturing beyond the ImageNet pre-training with class labels: <emph>e.g.</emph> <a href="https://paperswithcode.com/task/self-supervised-image-classification">self-supervision</a> and <a href="https://github.com/openai/CLIP">language-description supervision</a>.
          We believe now is a good time to discuss what’s next.
          The workshop will cover questions like:
          What are the main lessons learnt thanks to this benchmark?
          How can we reflect on the diverse requirements for good datasets and models, such as fairness, privacy, security, generalization, scale, and efficiency?
          What should the next generation of ImageNet-like benchmarks encompass?
          Through this workshop, we hope to collectively shape the landscape of the ML and CV research in the post-ImageNet era.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/tutorial.png" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#ffffce">Human Annotation</span>
        <span style="background-color:#c7ceea">Explainability</span>
        <span style="background-color:#e2c7e5">Evaluation</span>
        <br>
        <a href="https://www.youtube.com/watch?v=-cc2RYF37zE&list=PLcD_yLvcdUll95mAnBDV0rZKhfClJMZMr&index=1">
          <papertitle>Tutorial on Weakly-Supervised Learning in Computer Vision
          </papertitle>
        </a>
        <br>
        <a href="http://homepages.inf.ed.ac.uk/hbilen/">Hakan Bilen</a>,
        <a href="https://research.google/people/RodrigoBenenson/">Rodrigo Benenson</a>,
        <strong>Seong Joon Oh</strong>.
        <br>
        <em>ECCV</em>, 2020
        <br>
        <a href="https://hbilen.github.io/wsl-eccv20.github.io/">Website (slides)</a>
        <p>
          Deeply-learned computer vision models are data-hungry and manual annotations are expensive.
          Can we train models with “weaker” annotations?
          This tutorial provides an overview of the vast literature on weakly supervised learning methods in computer vision.
          We also discuss the limitations of current state-of-the-art methods and evaluation metrics.
          We propose future research directions that hopefully will spur disruptive progress in weakly supervised learning.
        </p>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/review.png" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <papertitle>Reviewing activities
        </papertitle>
        <br>
        <ul>
          <li>Serving as an area chair for NeurIPS & CVPR.</li>
          <li>Serving as a reviewer for CVPR, NeurIPS, ICML, ICCV, ECCV, ICLR, etc.</li>
          <li>6 x Best Reviewer Awards.</li>
        </ul>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/award.png" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <papertitle>Awards
        </papertitle>
        <br>
        <ul>
          <li>Samsung PhD Scholarship 2014-2018.</li>
          <li>Vensi Thawani Prize 2014: For a distinctive achievement in mathematics.</li>
          <li>William Pochin Scholarship 2014: For a distinctive achievement in mathematics.</li>
          <li>William Pochin Scholarship 2013: For the First Class honour in mathematics.</li>
          <li>Meritorious Winner 2012 at the Mathematical Contest in Modelling.</li>
          <li>William Pochin Scholarship 2011: For the First Class honour in mathematics.</li>
        </ul>
      </div>
    </div>

    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/talk.png" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9">
        <papertitle>Talks
        </papertitle>
        <br>
        <ul>
          <li>
            <a href="https://docs.google.com/presentation/d/19VLAJRzy13Zt2gvWpmO0Vqm5LQgvH8wbgB0-yCDwA_8/edit?usp=sharing">
              XAI for Scalable Trustworthy AI
            </a>.
            EML Workshop, Tübingen, March 2023.</li>
          <li>
            <a href="https://docs.google.com/presentation/d/10YClyhqK9s572L8QL9nNkfWbPzQlSKEAJ_moowI2yyU/edit?usp=sharing">
              Scalable Trustworthy AI
            </a>.
            AI Week, Tel Aviv, February 2023.</li>
          <li>
            <a href="https://docs.google.com/presentation/d/1BVyjRE-jq3cuVl9NM29WeFi6eUBTtPi6KzKRQufJr4g/edit?usp=sharing">
              Scalable Trustworthy AI -- Beyond "what", towards "how"
            </a>.
            SNU, Seoul, August 2022.</li>
          <li>
            Beyond "what", towards "how"
            <a href="https://docs.google.com/presentation/d/1a-dDfoKCHwMLCMa59lUrx1yGO5dbvMKlP7-nLO9PAAY/edit?usp=sharing">
              1/4
            </a>
            <a href="https://docs.google.com/presentation/d/16cUbeJuiMTTDT8aS4DR_kYVgiWyvcMDlx6LzxIu3fHg/edit?usp=sharing">
              2/4
            </a>
            <a href="https://docs.google.com/presentation/d/1rsktE9xNu2ScjY-bFD_wU1n0y13w6E8jhLi6HC3G3_o/edit?usp=sharing">
              3/4
            </a>
            <a href="https://docs.google.com/presentation/d/1BMUyw1pw-pUNzsTJpldF4JvXwlKJuuksVGaJSec_AZU/edit?usp=sharing">
              4/4.
            </a>
            UNIST, Virtual, September 2021.</li>
          <li>
            Beyond "what", towards "how"
            <a href="https://docs.google.com/presentation/d/1aphUrd4m1dSpU1xOdseo-ssL0xJjoZJKg6xe0RG1o7o/edit?usp=sharing">
              1/3
            </a>
            <a href="https://docs.google.com/presentation/d/1rGXLaNYIQYZvvnG_R9OFJk76z23pDq3L6mWvrDU0PbQ/edit?usp=sharing">
              2/3
            </a>
            <a href="https://docs.google.com/presentation/d/1G9Vls0nmTQHK-phFpxt9WTReFN8-78EjmXNIyZIcVk4/edit?usp=sharing">
              3/3.
            </a>
            Postech, Virtual, August 2021.</li>
          <li>
            <a href="https://docs.google.com/presentation/d/1QNWWBygLdiAvSG3N0z4i4xBfIOb5W7sFYoylIhdENfw/edit?usp=sharing">
              Beyond "what", towards "how".
            </a>
            University of Tübingen & MPI-IS, Virtual, January 2021.</li>
          <li>
            <a href="https://docs.google.com/presentation/d/1KhqlOq-cXWYGVJxBNx9EbwEUv6JPHM88Y7IhLsqEvG4/edit?usp=sharing">
              Towards reliable machine learning.
            </a>
            KAIST, Daejun, April 2020.</li>
          <li>
            <a href="https://docs.google.com/presentation/d/1KhqlOq-cXWYGVJxBNx9EbwEUv6JPHM88Y7IhLsqEvG4/edit?usp=sharing">
              Towards reliable machine learning.
            </a>
            IPIU, Jeju, Feb 2020.</li>
          <li>
            <a href="https://docs.google.com/presentation/d/1KhqlOq-cXWYGVJxBNx9EbwEUv6JPHM88Y7IhLsqEvG4/edit?usp=sharing">
              Towards reliable machine learning.
            </a>
            Azothbio, Seoul, January 2020.</li>
          <li>
            <a href="https://docs.google.com/presentation/d/1u4BbmAdvrz6HJH5dPVfxKf1Gd4K3zZ3hfnsUOAETyXE/edit?usp=sharing">
              Topics in ML research - security and uncertainty.
            </a>
            Postech, Pohang, June 2019.</li>
          <li>
            <a href="https://docs.google.com/presentation/d/1u4BbmAdvrz6HJH5dPVfxKf1Gd4K3zZ3hfnsUOAETyXE/edit?usp=sharing">
              Topics in ML research - security and uncertainty.
            </a>
            UNIST, Ulsan, May 2019.</li>
          <li>OCR in the wild. NeurIPS, Vancouver, December 2019.</li>
          <li>
            <a href="https://docs.google.com/presentation/d/1riTmAGDc-J5uO8kC_uL0ZnyNHDndpPmdaoOLAcfEqM0/edit?usp=sharing">
              Towards reverse-engineering deep neural networks.
            </a>
            Google, Mountain View, July 2018.</li>
          <li>Understanding and controlling user privacy in social media photos. KIST IMRC, Seoul, August 2017.</li>
          <li>Understanding and controlling user privacy in social media photos. MPI Intelligent Systems, T&uuml;bingen, June 2017.</li>
        </ul>
      </div>
    </div>
  </div>


  <div class="container">
    <div class="row">
      <div class="col">
        <p style="text-align:right;font-size:small;">
          Template based on <a href="https://jonbarron.info/">Jon Barron's website</a>.
        </p>
      </div>
    </div>
  </div>
</body>

</html>
